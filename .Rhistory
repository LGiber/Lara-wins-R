}
mscale <- function(x, from_min, from_max)
{
(as.matrix(x)-from_min)/(from_max-from_min)
#as.matrix(x)/from_max
}
mrescale <- function(x, min, max)
{
x*(max-min)+min
#x*max
}
forecast <- function(x, skyline=15)
{
trainingData <- makeTrainData(x)
n <- train(norm.func(trainingData))
for (i in 1:skyline)
{
res <- compute(n,mscale(trainingData[nrow(trainingData),2:6],min(trainingData),max(trainingData)))$net.result
h <- cbind(data.frame(trainingData[nrow(trainingData),2:6]),mrescale(res,min(trainingData),max(trainingData)))
colnames(h) <- colnames(trainingData)
trainingData <- rbind(trainingData,h)
}
plot(trainingData[,6], col="red")
points(x)
#data.frame(trainingData)
}
plot(trainingData[,6], col="red")
points(dollar)
plot(trainingData[,6], col="red")
points(x)
plot(trainingData[,6], col="red")
points(dollardata)
plot(trainingData[,6], col="red")
points(datadollar)
h <- cbind(data.frame(trainingData[nrow(trainingData),2:6]),mrescale(res,min(trainingData),max(trainingData)))
colnames(h) <- colnames(trainingData)
trainingData <- rbind(trainingData,h)
#print(h)
}
#print(mrescale(res,min(trainingData),max(trainingData)))
data.frame(trainingData)
}
forecastf(data_dollar)
res2 <- forecastf(data_dollar)
plot(res2[,6])
?plot
plot(cbind(res2[1,6],data_dollar))
plot(rbind(res2[1,6],data_dollar))
plot(data.frame(rbind(res2[1,6],data_dollar)))
plot(data.frame(rbind(res2[1,6],data_dollar)))
plot(res2[,6])
points(data_dollar)
plot(res2[,6])
points(data_dollar,col="red")
h <- cbind(data.frame(trainingData[nrow(trainingData),2:6]),mrescale(res,min(trainingData),max(trainingData)))
colnames(h) <- colnames(trainingData)
trainingData <- rbind(trainingData,h)
#print(h)
}
#print(mrescale(res,min(trainingData),max(trainingData)))
data.frame(trainingData)
}
forecastf(datadollar)
res2 <- forecastf(datadollar)
plot(res2[,6])
?plot
plot(cbind(res2[1,6],datadollar))
plot(rbind(res2[1,6],datadollar))
plot(data.frame(rbind(res2[1,6],datadollar)))
plot(data.frame(rbind(res2[1,6],datadollar)))
plot(res2[,6])
points(datadollar)
plot(res2[,6])
points(datadollar,col="red")
plot(res2[,6])
points(datadollar,col="red")
forecast(datadollar)
train <- function(trainingData)
{
neuralnet(getTrainFormula(ncol(trainingData)-1), data=trainingData, hidden = 10, threshold=0.01)
}
forecast(datadollar)
neuralnet
train <- function(trainingData)
{
neuralnet(getTrainFormula(ncol(trainingData)-1), data=trainingData, hidden = c(5,5), threshold=0.01)
}
norm.func <- function(x){
data.frame((x - min(x))/(max(x) - min(x)))
}
mscale <- function(x, from_min, from_max)
{
(as.matrix(x)-from_min)/(from_max-from_min)
}
mrescale <- function(x, min, max)
{
x*(max-min)+min
}
forecastf = function(x, skyline=15)
{
trainingData <- makeTrainData(x)
n <- train(norm.func(trainingData))
data_test2 <- matrix(nrow=1, ncol=ncol(trainingData)-1)
for (i in 1:5)
data_test2[1,i] <- trainingData[i,1]
for (i in 1:skyline)
{
res <- compute(n,mscale(trainingData[nrow(trainingData),2:6],min(trainingData),max(trainingData)))$net.result
h <- cbind(data.frame(trainingData[nrow(trainingData),2:6]),mrescale(res,min(trainingData),max(trainingData)))
colnames(h) <- colnames(trainingData)
trainingData <- rbind(trainingData,h)
}
data.frame(trainingData)
}
forecast (datadollar)
T <- seq(0,20,length=200)
Y <- 1 + 3*cos(4*T+2) +.2*T^2 + rnorm(200)
plot(T,Y,type="l")
install.packages("C:/Users/home/Downloads/forecast_5.9.zip", repos = NULL)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(object, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda, ...)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(object, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(object)
forecast(dataeuro)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(dataeuro)
forecast(dataeuro, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda,)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(dataeuro, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda,)
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
library("forecast", lib.loc="~/R/win-library/3.1")
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
install.packages("C:/Users/home/Downloads/zoo_1.7-11.zip", repos = NULL)
install.packages("C:/Users/home/Downloads/timeDate_3012.100.zip", repos = NULL)
library("forecast", lib.loc="~/R/win-library/3.1")
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
install.packages("C:/Users/home/Downloads/tseries_0.10-34.zip", repos = NULL)
install.packages("C:/Users/home/Downloads/fracdiff_1.4-2.zip", repos = NULL)
library("forecast", lib.loc="~/R/win-library/3.1")
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(dataeuro, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda,)
library("zoo", lib.loc="~/R/win-library/3.1")
library("Rcpp", lib.loc="~/R/win-library/3.1")
library("timeDate", lib.loc="~/R/win-library/3.1")
library("fracdiff", lib.loc="~/R/win-library/3.1")
library("forecast", lib.loc="~/R/win-library/3.1")
install.packages("C:/Users/home/Downloads/tseries_0.10-34.zip", repos = NULL)
library("tseries", lib.loc="~/R/win-library/3.1")
library("timeDate", lib.loc="~/R/win-library/3.1")
library("forecast", lib.loc="~/R/win-library/3.1")
nnetar(dataeuro, p, P=1, size, repeats=20, lambda=NULL)
## S3 method for class 'nnetar'
forecast(dataeuro, h=ifelse(object$m > 1, 2 * object$m, 10),
lambda=object$lambda,)
nnetar(data_euro, p, P=1, size, repeats=20, lambda=NULL)
nnetar(data_euro, 1, P=1, size, repeats=20, lambda=NULL)
nnetar(data_euro, p=1, P=1, size, repeats=20, lambda=NULL)
nnetar(data_euro, P=1, size, repeats=20, lambda=NULL)
nnetar(data_euro, P=1, repeats=20, lambda=NULL)
nnetar(data_euro,repeats=20, lambda=NULL)
nnetar(data_euro)
nn.sizes <- c(4,2,3,3,3,2,2,2)
numofsubs <- length(subsignals)
twindow <- 4
subsignals <- lapply(c(peakind$freqindex, midindex+1), function(x){
upperind <- x
fsub <- f
notnullind <- ((fsub$freqindex >= lowerind
& fsub$freqindex < upperind)
|
(fsub$freqindex >  (lindex - upperind + 2)
& fsub$freqindex <= (lindex - lowerind + 2)))
fsub[!notnullind,"coef"] <- 0
lowerind <<- upperind
Re(fft(fsub$coef, inverse=TRUE)/length(fsub$coef))
})
peakind <- f[abs(f$coef) > 3 & f$freqindex > 1 & f$freqindex < midindex,]
midindex <- ceiling((length(f$coef)-1)/ 2) + 1
lindex <- length(f$coef)
lowerind <- 1
subsignals <- lapply(c(peakind$freqindex, midindex+1), function(x){
upperind <- x
fsub <- f
notnullind <- ((fsub$freqindex >= lowerind
& fsub$freqindex < upperind)
|
(fsub$freqindex >  (lindex - upperind + 2)
& fsub$freqindex <= (lindex - lowerind + 2)))
fsub[!notnullind,"coef"] <- 0
lowerind <<- upperind
Re(fft(fsub$coef, inverse=TRUE)/length(fsub$coef))
})
nn.sizes <- c(4,2,3,3,3,2,2,2)
numofsubs <- length(data_euro)
twindow <- 4
offsettedsubdfs <- lapply(1:numofsubs, function(x){
singleoffsets <- lapply(0:(twindow-1), function(y){
data_euro[[x]][(twindow-y):(length(data_euro[[x]])-y-1)]
})
a <- Reduce(cbind, singleoffsets)
names <- lapply(1:twindow, function(y){paste("TS", as.character(x), "_", as.character(y), sep = "")})
b <- as.data.frame(a)
colnames(b) <- names
b
})
sample.number <- length(offsettedsubdfs[[1]][,1])
nns <- lapply(1:length(offsettedsubdfs), function(i)
{
nn <- nnet(offsettedsubdfs[[i]][1:(sample.number),], #the training samples
data_euro[[i]][(twindow+1):(length(data_euro[[i]]))], #the output
#corresponding to the training samples
size=nn.sizes[i], #number of neurons
maxit = 1000, #number of maximum iteration
linout = TRUE) #the neuron in the output layer should be linear
#the result of the trained networks should be plotted
plot(data_euro[[i]][(twindow+1):(length(data_euro[[i]]))], type="l")
lines(nn$fitted.values,type="l",col="red")
nn
})
library("nnet", lib.loc="C:/Program Files/R/R-3.1.2/library")
nns <- lapply(1:length(offsettedsubdfs), function(i)
{
nn <- nnet(offsettedsubdfs[[i]][1:(sample.number),], #the training samples
data_euro[[i]][(twindow+1):(length(data_euro[[i]]))], #the output
#corresponding to the training samples
size=nn.sizes[i], #number of neurons
maxit = 1000, #number of maximum iteration
linout = TRUE) #the neuron in the output layer should be linear
#the result of the trained networks should be plotted
plot(data_euro[[i]][(twindow+1):(length(data_euro[[i]]))], type="l")
lines(nn$fitted.values,type="l",col="red")
nn
})
number.of.predict <- 14
long.predictions <- lapply(1:length(offsettedsubdfs), function(i)
{
prediction <- vector(length=number.of.predict, mode="numeric")
#initial input
input <- offsettedsubdfs[[i]][sample.number,]
for (j in 1 : number.of.predict)
{
prediction[j] <- predict(nns[[i]], input)
input <- c(prediction[j],input[1:(length(input)-1)])
}
long.predictions <- lapply(1:length(offsettedsubdfs), function(i)
{
prediction <- vector(length=number.of.predict, mode="numeric")
#initial input
input <- offsettedsubdfs[[i]][sample.number,]
for (j in 1 : number.of.predict)
{
prediction[j] <- predict(nns[[i]], input)
input <- c(prediction[j],input[1:(length(input)-1)])
}
#we want to plot the prediction
plot(c(nns[[i]]$fitted.values,prediction), type="l",col="red")
lines(data_euro[[i]][(twindow+1):length(data_euro[[i]])])
prediction
})
plot(c(nns[[i]]$fitted.values,prediction), type="l",col="red")
lines(data_euro[[i]][(twindow+1):length(data_euro[[i]])]
nn.sizes <- c(4,2,3,3,3,2,2,2)
plot(c(nns[[i]]$fitted.values,prediction), type="l",col="red")
lines(data_euro[[i]][(twindow+1):length(data_euro[[i]])])
long.predictions <- lapply(1:length(offsettedsubdfs), function(i)
{
prediction <- vector(length=number.of.predict, mode="numeric")
#initial input
input <- offsettedsubdfs[[i]][sample.number,]
for (j in 1 : number.of.predict)
{
prediction[j] <- predict(nns[[i]], input)
input <- c(prediction[j],input[1:(length(input)-1)])
}
#we want to plot the prediction
plot(c(nns[[i]]$fitted.values,prediction), type="l",col="red")
lines(data_euro[[i]][(twindow+1):length(data_euro[[i]])])
prediction
})
install.packages(ggmap)
install.packages("C:/Users/home/Downloads/ggplot2_1.0.0.zip", repos = NULL)
install.packages("C:/Users/home/Downloads/ggmap_2.4.zip", repos = NULL)
gmap("Moscow Russia")
install.packages("C:/Users/home/Downloads/raster_2.3-24.zip", repos = NULL)
install.packages("C:/Users/home/Downloads/lattice_0.20-30.zip", repos = NULL)
install.packages("C:/Users/home/Downloads/sp_1.0-17.zip", repos = NULL)
library("sp", lib.loc="~/R/win-library/3.1")
library("lattice", lib.loc="~/R/win-library/3.1")
library("raster", lib.loc="~/R/win-library/3.1")
library("ggmap", lib.loc="~/R/win-library/3.1")
library("ggplot2", lib.loc="~/R/win-library/3.1")
gmap ("Moscow Russia")
ggmap ("Moscow Russia")
library("ggmap", lib.loc="~/R/win-library/3.1")
gadm__tp <- gadnm
install.packages("C:/Users/home/Downloads/RgoogleMaps_1.2.0.7.zip", repos = NULL)
library("RgoogleMaps", lib.loc="~/R/win-library/3.1")
install.packages("RColorBrewer")
install.packages("C:/Users/home/Downloads/png_0.1-7.zip", repos = NULL)
library("RgoogleMaps", lib.loc="~/R/win-library/3.1")
install.packages("RJSONIO")
library("RgoogleMaps", lib.loc="~/R/win-library/3.1")
library("ggmap", lib.loc="~/R/win-library/3.1")
library("ggplot2", lib.loc="~/R/win-library/3.1")
install.packages("jpeg")
library("ggmap", lib.loc="~/R/win-library/3.1")
install.packages("ggmap")
library("ggmap", lib.loc="~/R/win-library/3.1")
library("ggplot2", lib.loc="~/R/win-library/3.1")
ggmap ("moscow russia")
ggmap ("Moscow Russia")
?get_map
get_map ("Moscow Russia")
gmap("Moscow Russia")
get_cloudmademap("Mosow Russia")
??get_googlemap
library("raster", lib.loc="~/R/win-library/3.1")
library("twitteR", lib.loc="~/R/win-library/3.1")
library("twitteR")
install.packages("twitteR")
install.packages("twitteR")
load.twits(putin)
load_tweets(putin)
load_tweets_db(putin)
# search parameter
install.packages('XML')
require(XML)
mydata.vectors <- character(0)
for (page in c(1:15))
{
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q= ',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom '))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q= ',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom '))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q= ',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom '))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
length(mydata.vectors)
install.packages('tm')
require(tm)
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
mydata.corpus <- tm_map(mydata.corpus, tolower)
mydata.corpus <- tm_map(mydata.corpus, removePunctuation)
my_stopwords <- c(stopwords('english'), 'prolife', 'prochoice')
mydata.corpus <- tm_map(mydata.corpus, removeWords, my_stopwords)
mydata.dtm <- TermDocumentMatrix(mydata.corpus)
mydata.dtm
findFreqTerms(mydata.dtm, lowfreq=30)
###
### Read tweets from Twitter using ATOM (XML) format
###
# installation is required only required once and is rememberd across sessions
install.packages('XML')
# loading the package is required once each session
require(XML)
# initialize a storage variable for Twitter tweets
mydata.vectors <- character(0)
# paginate to get more tweets
for (page in c(1:15))
{
# search parameter
twitter_q <- URLencode('#prolife OR #prochoice')
# construct a URL
twitter_url = paste('http://search.twitter.com/search.atom?q= ',twitter_q,'&rpp=100&page=', page, sep='')
# fetch remote URL and parse
mydata.xml <- xmlParseDoc(twitter_url, asText=F)
# extract the titles
mydata.vector <- xpathSApply(mydata.xml, '//s:entry/s:title', xmlValue, namespaces =c('s'='http://www.w3.org/2005/Atom '))
# aggregate new tweets with previous tweets
mydata.vectors <- c(mydata.vector, mydata.vectors)
}
# how many tweets did we get?
length(mydata.vectors)
###
### Use tm (text mining) package
###
install.packages('tm')
require(tm)
# build a corpus
mydata.corpus <- Corpus(VectorSource(mydata.vectors))
# make each letter lowercase
mydata.corpus <- tm_map(mydata.corpus, tolower)
# remove punctuation
mydata.corpus <- tm_map(mydata.corpus, removePunctuation)
# remove generic and custom stopwords
my_stopwords <- c(stopwords('english'), 'prolife', 'prochoice')
mydata.corpus <- tm_map(mydata.corpus, removeWords, my_stopwords)
# build a term-document matrix
mydata.dtm <- TermDocumentMatrix(mydata.corpus)
# inspect the document-term matrix
mydata.dtm
# inspect most popular words
findFreqTerms(mydata.dtm, lowfreq=30)
install.packages("wordcloud")
library("twitteR")
library("wordcloud")
library("tm")
download.file(url="http://curl.haxx.se/ca/cacert.pem ", destfile="cacert.pem")
consumer_key <- 'TrviJZ5dHCmvHFEReiNa18RPX'
consumer_secret <- '1jMvdshbHXs8TSl9q3lKV9QhOkwEbGgfResJymyVyjlLk6DwTH'
access_token <- '213995172-Un1yzZGj8Geq693JTGLOGvsRHNdtsnNLeIMnCyVK'
access_secret <- 'PfwivqpYz7tXGHnPhn9Y7KPDUIXGJ0bBtOMqYDtQfIM7T'
setup_twitter_oauth(consumer_key,
consumer_secret,
access_token,
access_secret)
length(r_stats)
r_stats <- searchTwitter("#Putin", n=1500, cainfo="cacert.pem")
r_stats <- searchTwitter("#Putin", n=1500)
length(r_stats)
length(r_stats)
r_stats_text <- sapply(r_stats, function(x) x$getText())
r_stats_text_corpus <- Corpus(VectorSource(r_stats_text))
r_stats_text_corpus <- tm_map(r_stats_text_corpus, content_transformer(tolower))
r_stats_text_corpus <- tm_map(r_stats_text_corpus, removePunctuation)
r_stats_text_corpus <- tm_map(r_stats_text_corpus, function(x)removeWords(x,stopwords()))
wordcloud(r_stats_text_corpus)
r_stats1<- searchTwitter("#Putin", n=1500)
r_stats_text <- sapply(r_stats1, function(x) x$getText())
r_stats_text1 <- sapply(r_stats1, function(x) x$getText())
r_stats_text_corpus1 <- Corpus(VectorSource(r_stats_text1))
r_stats_text_corpus1 <- tm_map(r_stats_text_corpus1,
content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
mc.cores=1
)
r_stats_text_corpus1 <- tm_map(r_stats_text_corpus1, content_transformer(tolower), mc.cores=1)
r_stats_text_corpus1 <- tm_map(r_stats_text_corpus1, removePunctuation, mc.cores=1)
r_stats_text_corpus1 <- tm_map(r_stats_text_corpus1, function(x)removeWords(x,stopwords()), mc.cores=1)
r_stats_text_corpus1 <- tm_map(r_stats_text_corpus1, content_transformer(tolower), mc.cores=1)
wordcloud(r_stats_text_corpus)
wordcloud(r_stats_text_corpus1)
library(RColorBrewer)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(r_stats_text_corpus1,min.freq=2,max.words=100, random.order=T, colors=pal2)
source(“http://biostat.jhsph.edu/~jleek/code/twitterMap.R”)
get.packages(“twitterMap”)
install.packages(“twitterMap”)
twitterMap("RDataMining", fileName=”twitterMap.pdf”, nMax=1500)
install.packages(“twitterMap”)
install.packages(“http://biostat.jhsph.edu/~jleek/code/twitterMap.R”)
?source
source(http://biostat.jhsph.edu/~jleek/code/twitterMap.R)
source(http:\\biostat.jhsph.edu\~jleek\code\twitterMap.R)
source("http:\\biostat.jhsph.edu\~jleek\code\twitterMap.R")
source("http://biostat.jhsph.edu/~jleek/code/twitterMap.R")
twitterMap("navalny")
twitterMap("navalny", fileName=”twitterMap.pdf”, nMax=1500)
twitterMap("tolokno")
twitterMap("navalny", fileName=”twitterMap4.pdf”)
?twitterMap
twitterMap("tolokno",plotType="both")
twitterMap("tolokno",userLocation="Moscow")
twitterMap("tolokno",userLocation="Moscow", fileName=”twitterMap4.pdf”, plotType="both")
twitterMap("tolokno",userLocation="Moscow", plotType="both" , fileName=”twitterMap4.pdf” )
twitterMap("tolokno",userLocation="Moscow", fileName=”twitterMap4.pdf” )
twitterMap("tolokno",userLocation="Moscow", fileName= "twitterMap4.pdf" )
twitterMap("tolokno",userLocation="Moscow", fileName= "twitterMap4.pdf" , plotType="both" )
twitterMap("tolokno",userLocation="Moscow", fileName= "twitterMap6.pdf" , plotType="both" )
r_stats2 <- searchTwitter("#путин", n=1500)
rstats
r_stats
write.csv(r_stats.Rdata,"C:/Users/home/Desktop/R/griwe.csv")
write.csv(r_stats,"C:/Users/home/Desktop/R/griwe.csv")
write.csv(r_stats_text,"C:/Users/home/Desktop/R/griwe.csv")
r_stats_text_corpus
write.csv(r_stats1,"C:/Users/home/Desktop/R/griwe.csv")
?dataframe
??dataframe
write.csv(r_stats)
r_stats_text
